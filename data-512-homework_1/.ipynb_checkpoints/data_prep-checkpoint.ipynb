{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Article Page Views API Example\n",
    "This example illustrates how to access page view data using the [Wikimedia REST API](https://www.mediawiki.org/wiki/Wikimedia_REST_API). This example shows how to request monthly counts of page views for one specific article. The API documentation, [pageviews/per-article](https://wikimedia.org/api/rest_v1/#/Pageviews%20data), covers additional details that may be helpful when trying to use or understand this example.\n",
    "\n",
    "## License\n",
    "This code example was developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.1 - May 5, 2022\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/amrit/opt/anaconda3/envs/hcds/lib/python3.8/site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Users/amrit/opt/anaconda3/envs/hcds/lib/python3.8/site-packages (from pandas) (1.23.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/amrit/opt/anaconda3/envs/hcds/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/amrit/opt/anaconda3/envs/hcds/lib/python3.8/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/amrit/opt/anaconda3/envs/hcds/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /Users/amrit/opt/anaconda3/envs/hcds/lib/python3.8/site-packages (4.64.1)\n",
      "Collecting xarray\n",
      "  Downloading xarray-2022.9.0-py3-none-any.whl (943 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m943.1/943.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/amrit/opt/anaconda3/envs/hcds/lib/python3.8/site-packages (from xarray) (21.3)\n",
      "Requirement already satisfied: numpy>=1.19 in /Users/amrit/opt/anaconda3/envs/hcds/lib/python3.8/site-packages (from xarray) (1.23.3)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/amrit/opt/anaconda3/envs/hcds/lib/python3.8/site-packages (from xarray) (1.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/amrit/opt/anaconda3/envs/hcds/lib/python3.8/site-packages (from packaging>=20.0->xarray) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/amrit/opt/anaconda3/envs/hcds/lib/python3.8/site-packages (from pandas>=1.2->xarray) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/amrit/opt/anaconda3/envs/hcds/lib/python3.8/site-packages (from pandas>=1.2->xarray) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/amrit/opt/anaconda3/envs/hcds/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas>=1.2->xarray) (1.16.0)\n",
      "Installing collected packages: xarray\n",
      "Successfully installed xarray-2022.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "!pip install xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "# These are standard python modules\n",
    "import json, time, urllib.parse, os\n",
    "#\n",
    "# The 'requests' module is not a standard Python module. You will need to install this with pip/pip3 if you do not already have it\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import xarray as xa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example relies on some constants that help make the code a bit more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The REST API 'pageviews' URL - this is the common URL/endpoint for all 'pageviews' API requests\n",
    "API_REQUEST_PAGEVIEWS_ENDPOINT = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/'\n",
    "\n",
    "# This is a parameterized string that specifies what kind of pageviews request we are going to make\n",
    "# In this case it will be a 'per-article' based request. The string is a format string so that we can\n",
    "# replace each parameter with an appropriate value before making the request\n",
    "API_REQUEST_PER_ARTICLE_PARAMS = 'per-article/{project}/{access}/{agent}/{article}/{granularity}/{start}/{end}'\n",
    "\n",
    "# The Pageviews API asks that we not exceed 100 requests per second, we add a small delay to each request\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making a request to the Wikimedia API they ask that you include a \"unique ID\" that will allow them to\n",
    "# contact you if something happens - such as - your code exceeding request limits - or some other error happens\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<amb7896@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2022',\n",
    "}\n",
    "\n",
    "# read list of all dinosaur article titles\n",
    "dinosaur_wiki_df = pd.read_csv(\"/Users/amrit/Documents/Courses/Human Centered Data Science/Assignments/data-512-homework_1/dinosaur_genera.cleaned.SEPT.2022.csv\")\n",
    "# This is just a list of English Wikipedia article titles that we can use for example requests                            \n",
    "# ARTICLE_TITLES = [ 'Bison', 'Northern flicker', 'Red squirrel', 'Chinook salmon', 'Horseshoe bat' ]\n",
    "                               \n",
    "ARTICLE_TITLES = list(dinosaur_wiki_df.name)                               \n",
    "                               \n",
    "# This template is used to map parameter values into the API_REQUST_PER_ARTICLE_PARAMS portion of an API request. The dictionary has a\n",
    "# field/key for each of the required parameters. In the example, below, we only vary the article name, so the majority of the fields\n",
    "# can stay constant for each request. Of course, these values *could* be changed if necessary.\n",
    "ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE = {\n",
    "    \"project\":     \"en.wikipedia.org\",\n",
    "    \"access\":      \"desktop\",      # this should be changed for the different access types\n",
    "    \"agent\":       \"user\",\n",
    "    \"article\":     \"\",             # this value will be set/changed before each request\n",
    "    \"granularity\": \"monthly\",\n",
    "    \"start\":       \"2015010100\",\n",
    "    \"end\":         \"2022093000\"    # this is likely the wrong end date\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API request will be made using one procedure. The idea is to make this reusable. The procedure is parameterized, but relies on the constants above for the important parameters. The underlying assumption is that this will be used to request data for a set of article pages. Therefore the parameter most likely to change is the article_title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_pageviews_per_article(article_title = None, \n",
    "                                  endpoint_url = API_REQUEST_PAGEVIEWS_ENDPOINT, \n",
    "                                  endpoint_params = API_REQUEST_PER_ARTICLE_PARAMS, \n",
    "                                  request_template = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE,\n",
    "                                  headers = REQUEST_HEADERS):\n",
    "    # Make sure we have an article title\n",
    "    if not article_title: return None\n",
    "    \n",
    "    # Titles are supposed to have spaces replaced with \"_\" and be URL encoded\n",
    "    article_title_encoded = urllib.parse.quote(article_title.replace(' ','_'))\n",
    "    request_template['article'] = article_title_encoded\n",
    "    \n",
    "    # now, create a request URL by combining the endpoint_url with the parameters for the request\n",
    "    request_url = endpoint_url+endpoint_params.format(**request_template)\n",
    "    # print(request_url)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting data from Pageviews API and saving to avoid wasting time in redownloads if kernel crashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_access_modes = ['all-access', 'desktop', 'mobile-app', 'mobile-web']\n",
    "access_modes_dict = {}\n",
    "for i in range(len(device_access_modes)):\n",
    "    mode = device_access_modes[i]\n",
    "    ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE[\"access\"] = mode\n",
    "    mode_time_series = {}\n",
    "    # Used tqdm package to check download times of views data from Pageviews API\n",
    "    for j in tqdm(range(len(ARTICLE_TITLES))):\n",
    "        dino_article = ARTICLE_TITLES[j]\n",
    "        # print(f\"Getting pageview data for {dino_article} in {mode} mode\")\n",
    "        try:\n",
    "            views = request_pageviews_per_article(dino_article)\n",
    "            # Removing the ‘access’ field as it is misleading for mobile and cumulative files\n",
    "            dinosaur_views_wo_access_field = [ {key:item[key] for key in item if key!='access'} for item in views['items']]\n",
    "        except Exception as e:\n",
    "            print(views)\n",
    "        mode_time_series[dino_article] = dinosaur_views_wo_access_field\n",
    "    access_modes_dict[mode] = mode_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serializing json\n",
    "json_object = json.dumps(access_modes_dict, indent=4)\n",
    " \n",
    "# Save PageView API downloaded json to local to avoid redownload next time kernel dies\n",
    "with open(\"pageview_download.json\", \"w\") as outfile:\n",
    "    json.dump(access_modes_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pagview API downloaded data\n",
    "with open('pageview_download.json', 'r') as openfile:\n",
    " \n",
    "    # Reading from json file\n",
    "    access_modes_dict = json.load(openfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting dictionary data to dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summing mobile-app and mobile-web data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dinos_mobile_dict = {}\n",
    "for dino in access_modes_dict[\"mobile-app\"]:\n",
    "    dino_mobile_list = []\n",
    "    # TODO: Why was final list of time series objects consisting a replication of only the last time series object\n",
    "    # when not initialized?\n",
    "    # mobile_monthly_dict = {}\n",
    "    dino_all_months = access_modes_dict[\"mobile-app\"][dino]\n",
    "    for dino_mob_app_monthly_views in dino_all_months:\n",
    "        mobile_monthly_dict = {}\n",
    "        for key in dino_mob_app_monthly_views:\n",
    "            if key!='views':\n",
    "                mobile_monthly_dict[key] = dino_mob_app_monthly_views[key]\n",
    "            else:\n",
    "                filtered_dict = [ dino_mob_web_monthly_views for dino_mob_web_monthly_views in access_modes_dict[\"mobile-web\"][dino] \n",
    "                                            if dino_mob_web_monthly_views['timestamp'] == dino_mob_app_monthly_views['timestamp']]\n",
    "                mobile_monthly_dict[key] = dino_mob_app_monthly_views[key] + filtered_dict[0][\"views\"]\n",
    "        dino_mobile_list.append(mobile_monthly_dict)\n",
    "    all_dinos_mobile_dict[dino] = dino_mobile_list\n",
    "access_modes_dict['mobile'] = all_dinos_mobile_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create monthly mobile, desktop and cumulative files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dino_monthly_mobile_201507-202209.json\", \"w\") as outfile:\n",
    "    json.dump(access_modes_dict[\"mobile\"], outfile)\n",
    "    \n",
    "with open(\"dino_monthly_desktop_201507-202209.json\", \"w\") as outfile:\n",
    "    json.dump(access_modes_dict[\"desktop\"], outfile)\n",
    "    \n",
    "with open(\"dino_monthly_cumulative_201507-202209.json\", \"w\") as outfile:\n",
    "    json.dump(access_modes_dict[\"all-access\"], outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hop into dino_analysis.ipynb for Step2: Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_access_time_series_json = []\n",
    "mobile_access_modes_dict = access_modes_dict[\"mobile\"]\n",
    "for dino in mobile_access_modes_dict:\n",
    "    for time_series_list in mobile_access_modes_dict[dino]:\n",
    "        mobile_access_time_series_json.append(time_series_list)\n",
    "        \n",
    "desktop_access_time_series_json = []\n",
    "desktop_access_modes_dict = access_modes_dict[\"desktop\"]\n",
    "for dino in desktop_access_modes_dict:\n",
    "    for time_series_list in desktop_access_modes_dict[dino]:\n",
    "        desktop_access_time_series_json.append(time_series_list)\n",
    "        \n",
    "cumulative_access_time_series_json = []\n",
    "cumulative_access_modes_dict = access_modes_dict[\"all-access\"]\n",
    "for dino in cumulative_access_modes_dict:\n",
    "    for time_series_list in cumulative_access_modes_dict[dino]:\n",
    "        cumulative_access_time_series_json.append(time_series_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above output should show dictionaries with views per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
